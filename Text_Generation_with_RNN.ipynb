{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Text_Generation_with_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM7P7rDg60Ym15AGMRmOz5x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RahulSrinivas8/deeplearning/blob/main/Text_Generation_with_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5Ka4RHaDgSR"
      },
      "source": [
        "This tutorial demonstrates how to generate text using a character-based RNN.will work with a dataset of Shakespeare's writing from Andrej Karpathy's The Unreasonable Effectiveness of Recurrent Neural Networks. Given a sequence of characters from this data (\"Shakespear\"), train a model to predict the next character in the sequence (\"e\"). Longer sequences of text can be generated by calling the model repeatedly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpXil4WUGa6F"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PgrdeDqFRfT"
      },
      "source": [
        "# Import Tensorflow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrhirFuTDM2C"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7Pp6e3sGSVM"
      },
      "source": [
        "# Download the Shakespeare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKF9LLPaGPLN",
        "outputId": "b02c792c-c792-4839-ffc0-06f3e4131cfd"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt','https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nY9k1TdiHWKJ"
      },
      "source": [
        "# Read the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FVmddjwHefB",
        "outputId": "a092cc92-bd97-4def-d476-3e0f862b29b4"
      },
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmzey-b2IR10",
        "outputId": "143f5d0f-6a78-4179-8ffb-77ab0506c954"
      },
      "source": [
        "print(text[:250])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhinc-FsIe9N",
        "outputId": "613df153-a551-421f-e079-4fcd3a63780c"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzXS-z8sJIOX"
      },
      "source": [
        "#Process the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYtvZVRPJO1P"
      },
      "source": [
        "#Vectorise the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL85vXJbUOf2"
      },
      "source": [
        "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab), mask_token=None)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-v9QRMSVxnC"
      },
      "source": [
        "chars_from_ids = preprocessing.StringLookup(vocabulary = ids_from_chars.get_vocabulary(), invert=True, mask_token= None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9F6MnBiqYD8X"
      },
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dwC3KDGZr4y"
      },
      "source": [
        "#Create training examples and targets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nckKYW1XZvnn",
        "outputId": "305cbb64-8d5f-4ca5-c49e-3d921a5e5153"
      },
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiAR_Rj2fm27"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2hzcIBSfJuw",
        "outputId": "07d485eb-a007-4ff6-ce78-a67321a6dcaa"
      },
      "source": [
        "for ids in ids_dataset.take(15):\n",
        "  print(chars_from_ids(ids).numpy().decode('UTF-8'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n",
            "z\n",
            "e\n",
            "n\n",
            ":\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huqp8P07gEIb"
      },
      "source": [
        "seq_length =100\n",
        "examples_per_epoch = len(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NgJFGJThtnG",
        "outputId": "0ab587ce-9e79-497c-ee73-3dcbea31fc4e"
      },
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder= True)\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70gExz2_igg2"
      },
      "source": [
        "it's easier to see what this is doing if you join the tokens back into string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p20LHfpPiGzL",
        "outputId": "012f51ee-5286-4244-bc9f-be2d32461f60"
      },
      "source": [
        "for seq in sequences.take(4):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOxPWw8vi_p3"
      },
      "source": [
        "def split_input_target(sequences):\n",
        "  input_text =sequences[:-1]\n",
        "  target_text = sequences[1:]\n",
        "  return input_text, target_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pD17st7jiUx",
        "outputId": "17365e64-9fa1-4ddc-bb9c-17ab13f7b5dc"
      },
      "source": [
        "split_input_target(list(\"PYTHON\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['P', 'Y', 'T', 'H', 'O'], ['Y', 'T', 'H', 'O', 'N'])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAF9jncrjuz-"
      },
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpJr8cEUk3ty",
        "outputId": "345b099e-5538-4991-db37-a043f097a306"
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print(\"input :\",text_from_ids(input_example).numpy())\n",
        "  print(\"target :\",text_from_ids(target_example).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "target : b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVi19l-8loMU"
      },
      "source": [
        "# Creating Training Batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zouG-eSjltyk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cef12941-d2c4-4097-d227-de8946ac11e0"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEYgRGjoOTLu"
      },
      "source": [
        "# Build The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7f4FBqkOZWP"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "\n",
        "embedding_dim = 256\n",
        "\n",
        "rnn_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kI-2mHORJI1"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM7Zq99MoTtZ"
      },
      "source": [
        "model = MyModel(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6TsFIxRpb2X"
      },
      "source": [
        "#Try the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hcdnE_fpZGH",
        "outputId": "ceb4bbaa-c253-42fc-e722-3f7e7a962b6e"
      },
      "source": [
        "# (first check the shape of output\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE_nAcdZozwG",
        "outputId": "c2006ef3-4bc1-443c-d533-8d10c6eb0469"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  16896     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  67650     \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nd_I1wM9o7yx"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVCRVFmaqCK2",
        "outputId": "2a230ebd-0000-449c-ead7-3fd187e8fd9c"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([49, 10, 13,  5, 14, 20, 34, 14, 61, 22, 42, 40, 46, 28, 27, 39, 41,\n",
              "       57, 34, 57, 45,  8, 53, 57, 39, 11, 43, 33, 50, 25, 16,  6, 46, 61,\n",
              "       42, 55, 45, 22, 40,  3, 43,  5, 10, 53, 41, 21, 51, 42, 16, 46, 41,\n",
              "       30, 34,  6, 34, 52, 61, 63, 55, 63, 33, 16, 18, 31, 40, 32, 63, 63,\n",
              "        3, 11,  7, 61, 33, 56, 35,  1,  1, 58, 14,  4, 62,  5, 23, 22, 47,\n",
              "       51, 29, 15,  1, 53, 46, 29, 43, 40, 44, 30, 16, 30, 16,  2])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHyjoQIUqDyj",
        "outputId": "c8473275-23cf-4f06-99eb-a0fb27aa73fb"
      },
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'mberland.\\n\\nNORTHUMBERLAND:\\nThe commons will not then be satisfied.\\n\\nKING RICHARD II:\\nThey shall be s'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"j3?&AGUAvIcagONZbrUrf-nrZ:dTkLC'gvcpfIa!d&3nbHlcCgbQU'UmvxpxTCERaSxx!:,vTqV\\n\\nsA$w&JIhlPB\\nngPdaeQCQC \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXo7jiMGqQBZ"
      },
      "source": [
        "#Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOA2gUQgqMLu"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEP-e37NqXp4",
        "outputId": "79347175-d5aa-4018-b0ea-dfa3926b1494"
      },
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.1903486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb3KmGMZqjOD"
      },
      "source": [
        "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yG0meeyzqa5J",
        "outputId": "0c7d3e28-1789-4647-cd8d-6b02726f7a92"
      },
      "source": [
        "tf.exp(mean_loss).numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.045815"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYJKZYrZqoyo"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbP4VCBjq1fA"
      },
      "source": [
        "#Configure checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VLDyjnfq0oz"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA37vz94rOKa"
      },
      "source": [
        "#Execute the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r8OOuCFq7s3"
      },
      "source": [
        "EPOCHS = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjqoFbdyrUXu",
        "outputId": "9f3d03f0-76ee-4ff9-f6c4-3f3b6994018f"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 35s 184ms/step - loss: 2.7687\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 33s 183ms/step - loss: 2.0207\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 33s 183ms/step - loss: 1.7359\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 33s 182ms/step - loss: 1.5688\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 33s 182ms/step - loss: 1.4647\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 33s 181ms/step - loss: 1.3936\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 33s 181ms/step - loss: 1.3395\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 33s 181ms/step - loss: 1.2934\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 33s 180ms/step - loss: 1.2524\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 32s 180ms/step - loss: 1.2131\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 33s 181ms/step - loss: 1.1738\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 32s 180ms/step - loss: 1.1327\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 32s 179ms/step - loss: 1.0911\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 32s 177ms/step - loss: 1.0459\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 32s 179ms/step - loss: 0.9991\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 32s 180ms/step - loss: 0.9492\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 32s 178ms/step - loss: 0.8970\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 32s 178ms/step - loss: 0.8449\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 32s 180ms/step - loss: 0.7921\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 33s 182ms/step - loss: 0.7423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIn22zLqrbs2"
      },
      "source": [
        "#Generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgTeEb4arX9V"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9vW4kdPr3Hs"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf6QShfbr5kA",
        "outputId": "bbf243c1-9080-40f3-ef6c-b890d7e43d78"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Pourn if they do not much.\n",
            "\n",
            "SAMPSON:\n",
            "No, I come from Past self: for once would please your grace,\n",
            "Thry govern proclaim'd it offended and\n",
            "Destruction dagger, for your eye, that\n",
            "mights said well accept: a watery from his bed,\n",
            "And live an some fore, do me were so bootless.\n",
            "I heard you seize and slaughter'd aims at said\n",
            "Her and they the Ludlio's man, where we are gone, well, but\n",
            "Their new commanings no sovereign.\n",
            "\n",
            "KING HENRY VI:\n",
            "Why, thing you went! The oath of man or wounded nothing of\n",
            "thee thence from fortake to the crown o'clock.\n",
            "\n",
            "KING HENRY VI:\n",
            "Nay, take no man in him.\n",
            "\n",
            "RICHARD:\n",
            "And then, or else me now, speak from the hair\n",
            "Bohes my report to the will;\n",
            "For in a world so well aspections, to thy ears,\n",
            "Will rather sweat of breathed upon,\n",
            "And not unlikely like to fail in\n",
            "Dire-dieving kinsman, no chrenkncil arms\n",
            "We, how 'tis the murderer Provert: all's it is.\n",
            "\n",
            "KING RICHARD II:\n",
            "But when that mudder, long substitute,\n",
            "Hath an thus can peer the vault, nor honour of glives:\n",
            "One joins of the com \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.538032531738281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytGtWYHBszWf"
      },
      "source": [
        "If you want the model to generate text faster the easiest thing you can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nb4youQs3Ef",
        "outputId": "33747ea9-d3c0-42d4-9557-3da1ef9884e4"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nBlack for these good things to get you play.\\nAs is the commission fairest flowers o' the east,\\nYet thus thy boats we have but begun, prepare yourselves.\\n\\nKING RICHARD III:\\nGood morrow, tender of the duke.\\n\\nDUKE VINCENTIO:\\nUnbut a very little off,\\nThough you have made he fight as you with a life\\nThan dog wench; which of you would speed am not,\\nLest thou himself will break the heavy child being\\nMaliciously poison made thee provoked\\nA borm of light to main'd you in her eyes.\\nThey have most guarded; mistress beldow,\\nWhilst thou hast made thy weary? never shame for it\\nExceeding and stand to's a poor hell, friar.\\nBoth to my among moieth to you that\\nWhich hath as many dread: think it were, my kingry's books,\\nShe shall count him in your comfort, wherein he speaks\\nBeroaked in Padua shall have found: so do I;\\nI for any laboure upon't, and not too coll\\non her siste justful eye.\\nA fox, ord their courses are but thy soul with chastisement;\\nFor King of England's heirs and tokens to stead me.\\n\\nGLOUC\"\n",
            " b\"ROMEO:\\nWhy, what a like God is impossible.\\n\\nRoman:\\nThen, my son, these beggars here to curse the teachment, may saluties in.\\n\\nROMEO:\\nThis will I mean, seek up. Thine eyes shall crave their labour down with\\nBanished: but they'll prove a very sort.\\n\\nJOHN OF GAUNT:\\nNo, not forsworn himself: I'll not add more\\nThan they whose former roughs whom they walk and dissentio's master, hath destroy'd\\nThe manner in his feman in med like noble bed.\\n\\nPETER:\\nPrithee, no.\\n\\nANGELO:\\nPlease you that against him, fair belonged\\nconsul! while I pouse a cotyes fur me, and my name: when\\nyou cannot tell to most from delithed all that I am\\nEven son: 'twas ever man in parliament,\\nBeing restored eyes all ock.'\\n\\nGLOUCESTER:\\nYour eye glad that my dance fond goed against the tide\\nFor this good cause of heaven flocks.\\n\\nPOLIXENES:\\nMadam,\\nI have no son, the children yet unbod's mellow'd\\nHer bedren treasons: we'll hear your heart's life--\\nGremio hoppose his mellow'd than they are his.\\nTo my sweet loss, by my mother knows that i\"\n",
            " b\"ROMEO:\\nLet's be marry: 'tis tokens report of him.\\n\\nPOMPEY:\\nSir, you shall answer the gods. Come, sir, your gross;\\nAnd liberty, good sister; and, whiles well all formellous in!\\n\\nBUSHY:\\nThe six or sour lie, I came from an inch, Grey,\\nAnd spur'd me to my stuck and path.\\n\\nROMEO:\\nI pray yet now.\\n\\nMERCUTIO:\\nA horse! against thee hang'd you all so serviced,\\nYou, to remember\\nMurdering Buries, nor name of mercy.\\n\\nDUKE OF YORK:\\nBlind soon false and miserable man holds,\\nGiven honour! seept thou incerts my\\nMouthur lost at him, to make you speaking by\\nThy warlike Edward's champion.\\nSoft worthy sir.\\n\\nCORIOLANUS:\\nAy,\\nhere's him. Here's with me that illust mascals\\nFrom these old captivion.\\n\\nBAPTISTA:\\nWhat's he'r? a sovereign speake,--\\nThou shalt not svear; for tell me wretch,\\nAnd three a meril talk of tears, the other comes by new cries\\nThan that you rubble better: yet some slip be wish,\\nLest thou comes in Pedar'd hand and not\\nUntil the airy enemy, and tell stur,\\nOf comforts than any other good and open and\"\n",
            " b\"ROMEO:\\nAlas, you know, my sail; what my command?\\n\\nAposte:\\nThese arms upon hequest in her borous violence,\\nUnto the controversy to the head, the king's wife,\\nDivine honours, they have deposed, recounting\\nAs death is prisoner to the purpose: when you saw her\\neasier, Caius Marcius; and if\\nyou can worn by my mother's life,\\nAnd not to play the gate: come but our sweets,\\nAs he could draw to him than against his.\\nCan Warwick's wife and all the charm, how counterfeit\\nBuckingham lineament, and some profing\\nAnd holy pagew-booking once and die\\nTime of Hereford, will them to undergoon,\\nAnd please you then as far as doth acquit to them.\\n\\nBRUTUS:\\nMongord: will you we will\\nforgetly and a hopeful king: all is foolish banish'd arms.\\nCommend me to thy curse!\\n\\nGLOUCESTER:\\nNor I. O gentleman, and so see'truement,\\nOr modest as you hath not so loud,\\nThe resire of request and eyes, none cannot guess,\\nGrance, at Stanley, thy former uply all\\nfrail togetser and sleep cannow hath slain,\\nAnd not your flesh and witness \"\n",
            " b\"ROMEO:\\nWhat is't good? the king is very fines? certain I might be vow\\nA least may beside.\\n\\nFLORIZEL:\\nMy brief, lords?\\n\\nBlood:\\nBesides the attenged of his country tilk, and leap'd\\nHim stemb'd the head from Richard; there can speak die your\\nHave met us enough. eles, the place dead! take up my mother's\\npainted sweet as his sword than the Volsces wholesome as many diseases:\\nPut up your basing fair will off,\\nThe ranson's banishment. My restering hell!\\nHath he done with the policy Tybalt's pitch,\\nFor Edward what is yet upon the deed.\\n\\nQUEEN MARGARET:\\nThough fondage or I would not then?\\n\\nKING RICHARD II:\\nNorthumberland, the prisoner. The senate's son,\\nNow blest thy officers at arms in thine.\\n\\nGLOUCESTER:\\nMadam, had exercise, most tumbled silence! rests it not\\nThe mirth o' the malice and slept both them\\nbut; upon my house and your affairs, lest thy\\nwell-gent, and light to hood,--\\nNot of their shakes: whose backs that are thy cenker'd,\\nWill pursued thee and in mistress and preek,\\nWho trust our tarmen\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.210566759109497\n"
          ]
        }
      ]
    }
  ]
}